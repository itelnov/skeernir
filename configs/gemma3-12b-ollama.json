{
    "agent_graph": "ollama_server_chat.py",
    "host_name": "ollama",
    "model_name": "gemma3:12b",
    "sampling": {
        "temperature": 0.7,
        "num_ctx": 32000,
        "num_predict": -1
    }
}