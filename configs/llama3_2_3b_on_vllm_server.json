{   
    
    "server": "vllm",
    "model_path": "/docker_mount/data/gguf_models/Llama-3.2-3B-Instruct-Q8_0.gguf",
    "use_gpu": true,
    "max-model-len": 4096,
    "max-seq-len-to-capture": 4096,
    "task": "auto",
    "disable-frontend-multiprocessing": true,
    "sampling": {
        "temperature": 1.0,
        "top_p": 0.9,
        "top_k": -1,
        "min_p": 0.0,
        "repetition_penalty": 1.0,
        "presence_penalty": 0.0,
        "frequency_penalty": 0.0
    }
}